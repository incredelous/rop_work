\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvm}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvmfinalcopy
% \cvmfinalcopy % *** Uncomment this line for the final submission

\def\cvmPaperID{****} % *** Enter the cvm Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvmfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{A Template for CVM Proceedings}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\small\url{http://www.author.org/~second}}
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
    The Computational Visual Media Conference series, of which this is to be the first conference, is intended to provide a major new international forum for exchanging novel research ideas and significant practical results both underpinning and applying Visual Media.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}


\section{Related Work}
  Many auto or semi-auto dignose methods have been proposed to diagnose ROP plus disease. According to Aslam \etal~\cite{},  it takes three phrases to dignose,(1) image segmentation.With image processing algorithms to construct vascular tree; (2) measurement of vessel diameter. It is a challenge that describe vessel's thickness and location exactly, cause that the process is subjective, reliable; (3) measurement of tortuosity. with varied algorithms to evaluate tortuosity of blood vessel result in different results.

  Nowadays, cause of the excellent results Convolution Neural network(CNN) has achieved, more and more researchers start to focus on how to apply CNN on medical image process field, ROP images is no exception. Worrall \etal proposed a novel CNN architecture to diagnose ROP plus disease(waiting to complete). Brown \etal ~\cite{} use two CNN to diagnose ROP plus disease, the first CNN is vessel segmentation network, which is used to segment retinal vessel through output a probability map whose size is same as input image and the value is between 0 to 1. The second CNN is classification network whose architecture adopts Inception version1 ~\cite{} architecture.

  Above of all,
  Every method mentioned above is to to dignose the existence of ROP plus disease, or calssify normal, pre-plus, plus
  disease. In this article, we develop a novel neural network, through enhance the ability of feature map represent feature on every layer in CNN, which is described on the next section. with this network to diagnose the existence and severity of ROP gain state-of-the-art result.

\section{Data and Methodology}
\subsection{Data}
	Our data comes from Sichuang Provincial Peoples Hospital and Chengdu Women \& Childern's Central Hospital, which contains xxx ROP examinations from 2014 to 2017. Every examination consists of 4 to 12 retinal photographs, which reflects each premature infant's fundus situation.
  which annotated by two professional ophtahalmologists.

  First of all, we develop online label system to help ophtahalmologists annotate data, ophtahalmologist label on each examination, Figure 3.1 displays the detail of inspecting fundus photograph on label system, ophtahalmologist enable adjust brightness, saturation and contractness of retinal fundus photograph, and also we provide the draw line tools to aid ophtahalmologist caculate the distance between optic disc and "ridge/valley traversal", and the length of "ridge/valley traversal". Both of them assist ophtahalmologist diagnose the existence of ROP and severity of ROP certainly.

  Second, to ensure the consistence of data labels and prevent from false annotate result from annotator's carelessness, such as annotator's mishandle and lack of attention. we ask for three ophtahalmologists to annotate every ROP examination. one ophtahalmologist has more than 10 years clinical experience on ROP, and the other ophtahalmologists have about 5 years clinical experience relatively. We adopt examinations to construct dataset that the examinations have consistent label ammong three ophtahalmologists. Based on the theory~\cite{Alpher01}, retinal vessel tortuosity and the existence of "ridge/valley traversal" reflect the existence and severity of ROP. In order to construct a model mapping fundus image to ROP judgement, we need utilize the model to extract fundus image features. To better extract these features, we construct dataset with per-image rather than per-examination. (waiting to complete)

  Last, we divide all images into three sets, training sets, validation sets and testing sets respectively. Table 3.1 displays data distribution on every sets.

\subsection{Vessel Segmentation}
  Ophtahalmologist generally regard tortuosity of premature infants fundus image as one of most important standard to diagnose ROP, in fact, premature infants' retinal vessel haven't develop mature, when they separate themselves from maternal enviorment. And the infants unable to breath their own need artificial oxygen, which high concentration oxygen causes tortuosity of retinal vessel, become ROP finally.

  We try to utilize u-net~\cite{02} to segment vessel of fundus images. U-net is a symmetrical image segmentation framework, its left side consists of many modules serially, which every module has same construction, two $3\times3$ convolution operation used as feature extraction, one $2\times2$ max pooling operation used as downsampling. so does the right side, except for deconvolution operation is adopted to replace convolution operation. U-net uses skip connection fuse low level and high level features, which contributes to looking for dense and hierarchical image features.

  However, Annotating these fundus images to train segmentation model costs most resources.
  Consider that premature infants fundus image has part of similiarity with adult fundus image,"transfer learning" could assist we pre-train model, and then we fine tune parameters $\theta$ on segmentation model to access better segmentation performace.

\subsection{Classification Network}
  Since 2014, Google proposed Inception network architecture continuely~\cite{03, 04, 05, 06}, which imporved the best published result on ImageNet again and again. The successful secret of Inception network is a module named "Inception module" is proposed. Different from traniditional convolutional network, Inception network used a "Inception module"  as a layer. And the inventor of Inception network extend the width of "Inception module" with use different sizes of kernel to extract different spatial features, which superior than traniditional hierarchical convolutional network in performace.

  \emph{Median Frequency Balancing}: Consider that dataset is imbalanced, the number of normal samples is much more than ROP samples. We use median frequency balancing on loss function to deal with such problem. According to median frequency balancing, ${\alpha}_c$ denotes coordinate of class $c$ while training, \eg $ total loss = \sum_1^{n} {{{\alpha}_c} \dot loss(c)}$, which is formulated as:
  \begin{math}
    {\alpha}_c = \frac{median freq}{freq(c)}
  \end{math}

  $freq(c)$ denotes the number of class $c$ divided set number, and $median_freq$ is the median of all frequencies of classes. For binary classification tasks, we can formulate median frequency balancing as:
  \begin{math}
    {\alpha}_p = \frac{n + p}{2 \dot p}
    {\alpha}_n = \frac{n + p}{2 \dot n}
  \end{math}

  ${\alpha}_p$ denotes frequency of positive samples, ${\alpha}_n$ denotes frequency of negative samples, and $p, n$ denotes the number of positive and negative samples.

\subsection{Transfer Learning}
    "Transfer Learning" has been proved an effective method to train model, especially, the dataset is not very big. First of all, according to different tasks, we train classical popular models like InceptionV2 on big dataset, \eg ImageNet for image classification task, Pascal VOC dataset for image segmentation task, and CelebA for Face Detection. Secondly, we transfer parameters $\theta$ from these models to our own model respectively, through initializes $\theta$ on map function $f(y;x, \theta)$.

\subsection{Convolution and Pooling}


\subsection{Date}
Computational Visual Media Conference will be held on xxx to xxx.

\subsection{Language}
English is the official language of the conference.

\section{About the paper submission}

\subsection{Paper length}
cvm papers may be between 4 pages and 14 pages. Over length papers will simply not be reviewed.

%-------------------------------------------------------------------------
\subsection{Draft and final copy}
The \LaTeX\ style defines a printed ruler which should be present in the
version submitted for review.  The ruler is provided in order that
reviewers may comment on particular lines in the paper without
circumlocution. The camera ready copy should not contain a ruler.
(\LaTeX\ users may uncomment the \verb'\cvmfinalcopy' command in the document preamble.)

\subsection{Blind review}
Many authors misunderstand the concept of anonymizing for blind
review.  Blind review does not mean that one must remove
citations to one's own work---in fact it is often impossible to
review a paper unless the previous citations are known and
available.
Blind review means that you do not use the words ``my'' or ``our''
when citing previous work.

\begin{figure}[t]
\begin{center}
\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
   %\includegraphics[width=0.8\linewidth]{egfigure.eps}
\end{center}
   \caption{Example of caption.  }
\label{fig:long}
\label{fig:onecol}
\end{figure}

\subsection{Miscellaneous}

\noindent
Compare the following:\\
\begin{tabular}{ll}
 \verb'$conf_a$' &  $conf_a$ \\
 \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
\end{tabular}\\
See The \TeX book, p165.

The space after \eg, meaning ``for example'', should not be a
sentence-ending space. So \eg is correct, {\em e.g.} is not.  The provided
\verb'\eg' macro takes care of this.

When citing a multi-author paper, you may save space by using ``et alia'',
shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word.)
However, use it only when there are three or more authors.  Thus, the
following is correct: ``
   Frobnication has been trendy lately.
   It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
   Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...''
because reference~\cite{Alpher03} has just two authors.  If you use the
\verb'\etal' macro provided, then you need not worry about double periods
when used at the end of a sentence as in Alpher \etal.

For this citation style, keep multiple citations in numerical (not
chronological) order, so prefer \cite{Alpher03,Alpher02,Authors12} to
\cite{Alpher02,Alpher03,Authors12}.

%-------------------------------------------------------------------------
\subsection{References}

List and number all bibliographical references in 9-point Times,
single-spaced, at the end of your paper. When referenced in the text,
enclose the citation number in square brackets, for
example~\cite{Authors12}.  Where appropriate, include the name(s) of
editors of referenced books.

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Name & Performance \\
\hline\hline
A & OK\\
B & Bad \\
Ours & Great\\
\hline
\end{tabular}
\end{center}
\caption{An example for using tables.}
\end{table}

%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.  Please ensure that any point you wish to
make is resolvable in a printed copy of the paper.  Resize fonts in figures
to match the font in the body text, and choose line widths which render
effectively in print.  Many readers (and reviewers), even of an electronic
copy, will choose to print your paper in order to read it.  You cannot
insist that they do otherwise, and therefore must not assume that they can
zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use
\verb+\includegraphics+, and to specify the  figure width as a multiple of
the line width as in the example below
{\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
}


%-------------------------------------------------------------------------

{\small
\bibliographystyle{cvm}
\bibliography{cvmbib}
}


\end{document}
